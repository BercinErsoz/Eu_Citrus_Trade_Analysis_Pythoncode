{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd261acc-2c6b-44bd-9461-a6023c8deda7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c69a93d9-1c7b-4967-8b53-6cd900a087f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "file_names = [\n",
    "    'trhs201652.dat',\n",
    "    'trhs201752.dat',\n",
    "    'trhs201852.dat',\n",
    "    'trhs201952.dat',\n",
    "    'trhs202052.dat'\n",
    "]\n",
    "\n",
    "data_directory = '/Users/bercinersoz/Desktop/data 2/flows'\n",
    "\n",
    "# All files into a single DataFrame\n",
    "facttable_df = pd.concat([pd.read_csv(f\"{data_directory}/{file_name}\", delimiter=',') for file_name in file_names], ignore_index=True)\n",
    "facttable_df['PERIOD'] = facttable_df['PERIOD'].astype(str).str[:4]\n",
    "\n",
    "print(facttable_df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b6a60b4-5970-48b7-b5a1-7057b0d952b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(facttable_df.tail())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5585f22-0f1b-4cf5-86a9-76d53501bda9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display general information about the dataframe\n",
    "print(\"DataFrame Information:\")\n",
    "print(facttable_df.info())\n",
    "\n",
    "# Display summary statistics of the dataframe\n",
    "print(\"\\nSummary Statistics:\")\n",
    "print(facttable_df.describe(include='all'))\n",
    "\n",
    "# Display the number of missing values for each column\n",
    "print(\"\\nMissing Values:\")\n",
    "print(facttable_df.isnull().sum())\n",
    "\n",
    "# Display the data types of each column\n",
    "print(\"\\nData Types:\")\n",
    "print(facttable_df.dtypes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9bd604d8-a215-4b0e-a343-9a1fb129f31b",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "reporters_file = '/Users/bercinersoz/Desktop/data 2/metadata/REPORTERS.txt'\n",
    "partners_file = '/Users/bercinersoz/Desktop/data 2/metadata/PARTNERS.txt'\n",
    "cn_modified_file = '/Users/bercinersoz/Desktop/data 2/metadata/CN_modified.txt'\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c1572db-1029-48c9-ace1-7b7438e273ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "cn_modified_df = pd.read_csv(cn_modified_file, delimiter=',')\n",
    "cn_modified_df['HS2']=cn_modified_df['HS2'].astype(str).str.zfill(2)  #problem solved in HS2 0 to 00\n",
    "print(\"CN DataFrame:\")\n",
    "print(cn_modified_df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ae37040-7dda-4d13-8e98-89da7c7169a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "keywords = ['orange', 'tangerine', 'lemon', 'grapefruit', 'citrus']\n",
    "\n",
    "# Convert HS6_NAME to lowercase for case-insensitive search\n",
    "cn_modified_df['HS6_NAME'] = cn_modified_df['HS6_NAME'].str.lower()\n",
    "\n",
    "# Find rows where HS6_NAME contains any of the keywords\n",
    "citrus_products_df = cn_modified_df[cn_modified_df['HS6_NAME'].str.contains('|'.join(keywords))]\n",
    "\n",
    "# Extract the relevant columns\n",
    "citrus_hs6_codes = citrus_products_df[['HS6', 'HS6_NAME', 'HS4_NAME', 'HS2_NAME']]\n",
    "\n",
    "citrus_hs6_codes.to_csv('/Users/bercinersoz/Desktop/citrus_products_filtered.csv', index=False)\n",
    "\n",
    "# HS6 codes to exclude\n",
    "exclude_codes = ['081090', '081340', '200799', '200980', '200989', '282420', '330129']\n",
    "\n",
    "# Filter out\n",
    "citrus_hs6_codes = citrus_hs6_codes[~citrus_hs6_codes['HS6'].isin(exclude_codes)]\n",
    "\n",
    "print(\"Citrus Products and their HS6 Codes:\")\n",
    "print(citrus_hs6_codes)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7de4a37-dc68-4e1b-bfbc-35ceedfe3ebb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter facttable_df, without copy gives warning!\n",
    "filtered_facttable_df = facttable_df[facttable_df['PRODUCT_HS'].isin(citrus_hs6_codes['HS6'])].copy()\n",
    "\n",
    "# Ensure Product HS is a string\n",
    "filtered_facttable_df['PRODUCT_HS'] = filtered_facttable_df['PRODUCT_HS'].astype(str)\n",
    "cn_modified_df['HS6'] = cn_modified_df['HS6'].astype(str)\n",
    "\n",
    "# Perform a left join on PRODUCT_HS\n",
    "filtered_facttable_df = filtered_facttable_df.merge(cn_modified_df[['HS6', 'HS4_NAME']], left_on='PRODUCT_HS', right_on='HS6', how='left')\n",
    "\n",
    "unique_hs4_names = filtered_facttable_df['HS4_NAME'].unique()\n",
    "\n",
    "print(\"Unique HS4_NAME values:\")\n",
    "print(unique_hs4_names)\n",
    "\n",
    "# Truncate HS4_NAME to the first 30 characters\n",
    "filtered_facttable_df.loc[:, 'HS4_NAME'] = filtered_facttable_df['HS4_NAME'].str[:30]\n",
    "\n",
    "# Drop the HS6 column\n",
    "filtered_facttable_df = filtered_facttable_df.drop(columns=['HS6'])\n",
    "\n",
    "print(filtered_facttable_df.head())\n",
    "\n",
    "filtered_facttable_df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e514e894-6fd0-4ff1-a369-816bc18c8199",
   "metadata": {},
   "outputs": [],
   "source": [
    "reporters_df = pd.read_csv(reporters_file, delimiter=\"\\t\", names=[\"ID\", \"Date_since_declarent\", \"Date_until_declarent\", \"Name\", \"Date_since_declarent2\", \"Date_until_declarent2\"], encoding='ISO-8859-1')\n",
    "reporters_df = reporters_df.iloc[:, 0:4]\n",
    "reporters_df[\"ID\"] = reporters_df[\"ID\"].str.lstrip('0')\n",
    "reporters_df[\"Name\"] = reporters_df[\"Name\"].str.strip()\n",
    "\n",
    "partners_df = pd.read_csv(partners_file, delimiter=\"\\t\", names=[\"ID\", \"Date_since_partner\", \"Date_until_partner\", \"Name\", \"Date_since_partner2\", \"Date_until_partner2\"], encoding='ISO-8859-1')\n",
    "partners_df = partners_df.iloc[:, 0:4]\n",
    "partners_df[\"ID\"] = partners_df[\"ID\"].str.lstrip('0')\n",
    "partners_df[\"Name\"] = partners_df[\"Name\"].str.strip()\n",
    "\n",
    "# Exist (unique, between 31.12.2015-31.12.2020)\n",
    "reporters_exits = reporters_df.groupby('Name').filter(lambda x: len(x) == 1 and '31/12/2015' < x['Date_until_declarent'].iloc[0] <= '31/12/2020')\n",
    "partners_exits = partners_df.groupby('Name').filter(lambda x: len(x) == 1 and '31/12/2015' < x['Date_until_partner'].iloc[0] <= '31/12/2020')\n",
    "\n",
    "# Entrance (unique, between 31.12.2015-31.12.2020)\n",
    "reporters_entries = reporters_df.groupby('Name').filter(lambda x: len(x) == 1 and x['Date_since_declarent'].iloc[0] > '31/12/2015')\n",
    "partners_entries = partners_df.groupby('Name').filter(lambda x: len(x) == 1 and x['Date_since_partner'].iloc[0] > '31/12/2015')\n",
    "\n",
    "reporters_exits_countries = reporters_exits[['Name', 'Date_until_declarent']].drop_duplicates()\n",
    "partners_exits_countries = partners_exits[['Name', 'Date_until_partner']].drop_duplicates()\n",
    "\n",
    "reporters_entries_countries = reporters_entries[['Name', 'Date_since_declarent']].drop_duplicates()\n",
    "partners_entries_countries = partners_entries[['Name', 'Date_since_partner']].drop_duplicates()\n",
    "\n",
    "print(\"Reporters Exits Countries (31.12.2015 - 31.12.2020):\")\n",
    "print(reporters_exits_countries)\n",
    "\n",
    "print(\"Partners Exits Countries (31.12.2015 - 31.12.2020):\")\n",
    "print(partners_exits_countries)\n",
    "\n",
    "print(\"Reporters Entries Countries (After 31.12.2015):\")\n",
    "print(reporters_entries_countries)\n",
    "\n",
    "print(\"Partners Entries Countries (After 31.12.2015):\")\n",
    "print(partners_entries_countries)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4a72331-d688-4deb-b7c3-fcd606fdc92e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter out GB from declarant countries\n",
    "filtered_facttable_df = filtered_facttable_df[filtered_facttable_df['DECLARANT_ISO'] != 'GB']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff640135-cc50-4d78-a194-d912d952110e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "world_bank_gdp_file = '/Users/bercinersoz/Desktop/data 2/worldbank_gdp.csv'\n",
    "\n",
    "# Read the World Bank GDP data and select only the specified columns\n",
    "columns_to_keep = ['Country Name', 'Country Code', '2016 [YR2016]', '2017 [YR2017]', '2018 [YR2018]', '2019 [YR2019]']\n",
    "world_bank_gdp_df = pd.read_csv(world_bank_gdp_file, usecols=columns_to_keep, encoding='ISO-8859-1')\n",
    "\n",
    "world_bank_gdp_df.columns = ['Country Name', 'Country Code', '2016', '2017', '2018', '2019']\n",
    "\n",
    "world_bank_gdp_df['Country Code'] = world_bank_gdp_df['Country Code'].str.strip()  #problem solved to unmatch case\n",
    "\n",
    "# Convert GDP columns to numeric (float)\n",
    "for year in ['2016', '2017', '2018', '2019']:\n",
    "    world_bank_gdp_df[year] = pd.to_numeric(world_bank_gdp_df[year], errors='coerce')\n",
    "\n",
    "# Define the USD to Euro conversion rates for the respective years based on ECB data\n",
    "usd_to_euro_rates = {\n",
    "    '2016': 0.9048,\n",
    "    '2017': 0.8852,\n",
    "    '2018': 0.8471,\n",
    "    '2019': 0.8933\n",
    "}\n",
    "\n",
    "# Convert GDP values from USD to Euro\n",
    "for year in ['2016', '2017', '2018', '2019']:\n",
    "    world_bank_gdp_df[year] = world_bank_gdp_df[year] * usd_to_euro_rates[year]\n",
    "\n",
    "# Melt the DataFrame to long format\n",
    "melted_gdp_df = pd.melt(world_bank_gdp_df, id_vars=['Country Name', 'Country Code'], var_name='Year', value_name='GDP')\n",
    "\n",
    "print(\"Data types after conversion:\")\n",
    "print(melted_gdp_df.dtypes)\n",
    "\n",
    "print(\"Melted World Bank GDP DataFrame (in Euro):\")\n",
    "print(melted_gdp_df.head())\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6717032-c67f-415e-9b22-4edfdf9e68e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "melted_gdp_df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f26b8ff4-10d8-49f1-8633-bc56f8c8dce3",
   "metadata": {},
   "outputs": [],
   "source": [
    "transport_mode_data = {\n",
    "    'ID': [0, 1, 2, 3, 4, 5, 7, 8, 9],\n",
    "    'TRANSPORTATION': [\n",
    "        'Unknown', 'Sea', 'Rail', 'Road', 'Air', 'Post', \n",
    "        'Fixed Mechanism', 'Inland Waterway', 'Self Propulsion'\n",
    "    ]\n",
    "}\n",
    "\n",
    "# Create a DataFrame from the transport mode data\n",
    "transport_mode_df = pd.DataFrame(transport_mode_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2609688-d6a3-455e-bfd5-6a149b295aab",
   "metadata": {},
   "outputs": [],
   "source": [
    "flow_data = {\n",
    "    'ID': [1, 2],\n",
    "    'TRADE_FLOW': ['Import', 'Export']\n",
    "}\n",
    "\n",
    "# Create a DataFrame from the flow data\n",
    "flow_df = pd.DataFrame(flow_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8479320-233c-4a82-93d3-35d0de1b3028",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(flow_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6bf9f054-4ade-4210-b30a-3dc9a1b1ed80",
   "metadata": {},
   "outputs": [],
   "source": [
    "iso_codes_file = '/Users/bercinersoz/Desktop/data 2/metadata/iso_codes.csv'\n",
    "\n",
    "iso_codes_df = pd.read_csv(iso_codes_file, delimiter=\";\", usecols=['iso2', 'iso3'])\n",
    "\n",
    "print(\"ISO Codes DataFrame:\")\n",
    "print(iso_codes_df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ea3351c-d489-4a4c-ac4c-0957b5a8d862",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "iso2_to_iso3 = iso_codes_df.set_index('iso2')['iso3'].to_dict()\n",
    "\n",
    "# Ensure the key columns are of the same type and correct any potential issues with column names\n",
    "filtered_facttable_df.loc[:, 'DECLARANT_ISO'] = filtered_facttable_df['DECLARANT_ISO'].astype(str)\n",
    "filtered_facttable_df.loc[:, 'PARTNER_ISO'] = filtered_facttable_df['PARTNER_ISO'].astype(str)\n",
    "\n",
    "# Map DECLARANT_ISO and PARTNER_ISO to their 3-digit equivalents using the mapping dictionary\n",
    "filtered_facttable_df.loc[:, 'DECLARANT_ISO_3'] = filtered_facttable_df.loc[:, 'DECLARANT_ISO'].map(iso2_to_iso3)\n",
    "filtered_facttable_df.loc[:, 'PARTNER_ISO_3'] = filtered_facttable_df.loc[:, 'PARTNER_ISO'].map(iso2_to_iso3)\n",
    "\n",
    "# Drop the original DECLARANT_ISO and PARTNER_ISO columns\n",
    "filtered_facttable_df = filtered_facttable_df.drop(columns=['DECLARANT_ISO', 'PARTNER_ISO'])\n",
    "\n",
    "print(\"Updated Filtered Fact Table with 3-digit ISO codes:\")\n",
    "print(filtered_facttable_df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81ad0365-c9dd-446f-9832-528b19ad31bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Perform a left join on FLOW \n",
    "merged_df = filtered_facttable_df.merge(flow_df, left_on='FLOW', right_on='ID', how='left', suffixes=('', '_flow'))\n",
    "\n",
    "# Perform a left join on TRANSPORT_MODE\n",
    "merged_df = merged_df.merge(transport_mode_df, left_on='TRANSPORT_MODE', right_on='ID', how='left', suffixes=('', '_transport_mode'))\n",
    "\n",
    "print(\"Filtered Fact Table Columns:\")\n",
    "print(merged_df.columns)\n",
    "\n",
    "# Drop the redundant ID columns from flow and transport mode merges\n",
    "merged_df = merged_df.drop(columns=[ 'FLOW', 'TRANSPORT_MODE', 'ID', 'ID_transport_mode'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b278bf70-5555-4de7-8236-fd718dbada76",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ensure the key columns are of the same type\n",
    "merged_df['DECLARANT'] = merged_df['DECLARANT'].astype(str)\n",
    "merged_df['PARTNER'] = merged_df['PARTNER'].astype(str)\n",
    "\n",
    "# Join with reporters_df to get declarant country names\n",
    "merged_df = merged_df.merge(reporters_df[['ID', 'Name']], left_on='DECLARANT', right_on='ID', how='left', suffixes=('', '_declarant'))\n",
    "merged_df = merged_df.rename(columns={'Name': 'DECLARANT_COUNTRY'})\n",
    "\n",
    "# Join with partners_df to get partner country names\n",
    "merged_df = merged_df.merge(partners_df[['ID', 'Name']], left_on='PARTNER', right_on='ID', how='left', suffixes=('', '_partner'))\n",
    "merged_df = merged_df.rename(columns={'Name': 'PARTNER_COUNTRY'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7132edfd-d8cb-45bd-9ba8-d50dde68e9e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop the additional ID columns\n",
    "merged_df = merged_df.drop(columns=['ID', 'ID_partner'])\n",
    "print(\"Merged DataFrame:\")\n",
    "print(merged_df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d19a6a2d-1e69-46dd-bb94-05ac3703467a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the desired column order\n",
    "column_order = [\n",
    "    'DECLARANT', 'DECLARANT_COUNTRY', 'DECLARANT_ISO_3', 'PARTNER', 'PARTNER_COUNTRY', 'PARTNER_ISO_3', \n",
    "    'TRADE_FLOW', 'TRANSPORTATION', 'PERIOD', 'PRODUCT_HS', 'HS4_NAME', 'VALUE_IN_EUROS', 'QUANTITY_IN_KG'\n",
    "]\n",
    "\n",
    "# Reorder the columns in the DataFrame\n",
    "merged_df = merged_df[column_order]\n",
    "\n",
    "print(\"Reordered DataFrame:\")\n",
    "print(merged_df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f159641-70a9-431a-b9ef-d7e439ecb5c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"DataFrame Information:\")\n",
    "print(merged_df.info())\n",
    "\n",
    "print(\"\\nSummary Statistics:\")\n",
    "print(merged_df.describe(include='all'))\n",
    "\n",
    "print(\"\\nMissing Values:\")\n",
    "print(merged_df.isnull().sum())\n",
    "\n",
    "print(\"\\nData Types:\")\n",
    "print(merged_df.dtypes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1940a0ff-7b88-41e6-b6da-89d0f05bb41b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find rows where both QUANTITY_IN_KG and VALUE_IN_EUROS are 0\n",
    "zero_both_df = merged_df[(merged_df['QUANTITY_IN_KG'] == 0) & (merged_df['VALUE_IN_EUROS'] == 0)]\n",
    "\n",
    "zero_both_count = zero_both_df.shape[0]\n",
    "\n",
    "print(f\"Total rows with both QUANTITY_IN_KG and VALUE_IN_EUROS equal to 0: {zero_both_count}\")\n",
    "\n",
    "# Find rows where VALUE_IN_EUROS is 0\n",
    "zero_value_df = merged_df[merged_df['VALUE_IN_EUROS'] == 0]\n",
    "\n",
    "print(zero_value_df.head())\n",
    "\n",
    "zero_value_count = zero_value_df.shape[0]\n",
    "\n",
    "print(f\"Total rows with VALUE_IN_EUROS equal to 0: {zero_value_count}\")\n",
    "\n",
    "# Find rows where QUANTITY_IN_KG is 0\n",
    "zero_quantity_df = merged_df[merged_df['QUANTITY_IN_KG'] == 0]\n",
    "\n",
    "print(zero_quantity_df.head())\n",
    "\n",
    "zero_quantity_count = zero_quantity_df.shape[0]\n",
    "\n",
    "print(f\"Total rows with QUANTITY_IN_KG equal to 0: {zero_quantity_count}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4f5dff6-db92-49d5-a939-39c0367853a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "filtered_df=merged_df.copy()\n",
    "\n",
    "# Display rows where PARTNER_ISO_3 is missing\n",
    "missing_partner_iso_3 = filtered_df[filtered_df['PARTNER_ISO_3'].isna()]\n",
    "\n",
    "print(\"Rows with missing PARTNER_ISO_3 values:\")\n",
    "print(missing_partner_iso_3.head())\n",
    "print(f\"Total missing values in PARTNER_ISO_3: {missing_partner_iso_3.shape[0]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b2bafb2-5152-456c-a7d2-988968709fa7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find distinct NAME_partner values\n",
    "distinct_name_partner = missing_partner_iso_3['PARTNER_COUNTRY'].dropna().unique()\n",
    "\n",
    "print(f\"Total distinct NAME_partner values with missing PARTNER_ISO_3: {len(distinct_name_partner)}\")\n",
    "print(\"Distinct NAME_partner values with missing PARTNER_ISO_3:\")\n",
    "for name in distinct_name_partner:\n",
    "    print(name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76b6975b-9e0c-49e2-8587-a2f70cdab978",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "filtered_df['PARTNER_COUNTRY'] = filtered_df['PARTNER_COUNTRY'].str.strip()\n",
    "\n",
    "# Additional cleaning for specific cases\n",
    "filtered_df['PARTNER_COUNTRY'] = filtered_df['PARTNER_COUNTRY'].replace({\n",
    "    'Countries and territories not specified within the framework of trade with third countries': 'Not specified (third countries)',\n",
    "    'Stores and Provisions, Extra': 'Not specified (third countries)',\n",
    "    'Stores and provisions within the framework of trade with third countries': 'Not specified (third countries)',\n",
    "    'High seas': 'Not specified (third countries)',\n",
    "    'Countries and territories not specified for commercial or military reasons in the framework of trade with third countries': 'Not specified (third countries)',\n",
    "    \n",
    "})\n",
    "\n",
    "iso3_mapping = {\n",
    "    'Liechtenstein': 'LIE',\n",
    "    'Kosovo': 'XKX',\n",
    "    'Montenegro': 'MNE',\n",
    "    'Serbia': 'SRB',\n",
    "    'Congo, Democratic Republic of': 'COD',\n",
    "    'Congo,  Democratic Republic of': 'COD',\n",
    "    'Curaçao': 'CUW',\n",
    "    'Sint Maarten (Dutch part)': 'SXM',\n",
    "    'South Sudan': 'SSD',\n",
    "    'Namibia': 'NAM',\n",
    "    'Bonaire, Sint Eustatius and Saba': 'BES',\n",
    "    'Saint Barthélemy': 'BLM',\n",
    "    'Timor-Leste': 'TLS',\n",
    "    'Antarctica': 'ATA',\n",
    "    'Guam': 'GUM',\n",
    "    'Holy See (Vatican City State)': 'VAT',\n",
    "    'American Samoa': 'ASM',\n",
    "    'Ceuta': 'XC',\n",
    "    'Melilla': 'ML',\n",
    "    'Canary Islands': 'IC',\n",
    "    'Virgin Islands, United States': 'VIR',\n",
    "    'Not specified (third countries)': 'XXX'\n",
    "}\n",
    "\n",
    "# Update the PARTNER_ISO_3 column based on the iso3_mapping\n",
    "for partner_name, iso3_code in iso3_mapping.items():\n",
    "    filtered_df.loc[filtered_df['PARTNER_COUNTRY'] == partner_name, 'PARTNER_ISO_3'] = iso3_code\n",
    "\n",
    "# Check which rows still have missing PARTNER_ISO_3\n",
    "missing_partner_iso_3 = filtered_df[filtered_df['PARTNER_ISO_3'].isna()]\n",
    "missing_count = missing_partner_iso_3.shape[0]\n",
    "distinct_name_partner_missing = missing_partner_iso_3['PARTNER_COUNTRY'].dropna().unique()\n",
    "\n",
    "print(f\"Total rows with missing PARTNER_ISO_3: {missing_count}\")\n",
    "print(\"Distinct PARTNER_COUNTRY values with missing PARTNER_ISO_3:\")\n",
    "print(distinct_name_partner_missing)\n",
    "\n",
    "# Final DataFrame\n",
    "final_df = filtered_df.copy()\n",
    "\n",
    "print(\"Final DataFrame:\")\n",
    "print(final_df.head())\n",
    "print(f\"Total rows in the final DataFrame: {final_df.shape[0]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "144db00a-a2ee-4731-b722-bafe73d4163e",
   "metadata": {},
   "outputs": [],
   "source": [
    "csv_file_path = '/Users/bercinersoz/Desktop/countries_codes_and_coordinates.csv'\n",
    "excel_file_path = '/Users/bercinersoz/Desktop/final_data_with_coordinates.xlsx'\n",
    "\n",
    "coordinates_df = pd.read_csv(csv_file_path)\n",
    "\n",
    "# Remove double quotes in all cells\n",
    "coordinates_df = coordinates_df.applymap(lambda x: x.replace('\"', '') if isinstance(x, str) else x)\n",
    "\n",
    "coordinates_df['Latitude (average)'] = pd.to_numeric(coordinates_df['Latitude (average)'], errors='coerce')\n",
    "coordinates_df['Longitude (average)'] = pd.to_numeric(coordinates_df['Longitude (average)'], errors='coerce')\n",
    "\n",
    "with pd.ExcelWriter(excel_file_path, engine='openpyxl') as writer:\n",
    "    final_df.to_excel(writer, sheet_name='Final Data', index=False)\n",
    "    melted_gdp_df.to_excel(writer, sheet_name='GDP Data', index=False)\n",
    "    coordinates_df.to_excel(writer, sheet_name='Country Codes', index=False)\n",
    "\n",
    "print(f\"DataFrames have been written to {excel_file_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5f47bba-d50f-4297-9779-5092e04cfb5c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "merged_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a1211f8-24a1-4338-9519-d0b7129227da",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter the dataframe for Spain as the declarant and for the year 2020\n",
    "spain_2020_df = final_df[(final_df['DECLARANT_ISO_3'] == 'ESP') & (final_df['PERIOD'] == '2020') & (final_df['TRADE_FLOW'] == 'Export')]\n",
    "\n",
    "print(spain_2020_df.head())\n",
    "\n",
    "\n",
    "# Calculate the total Value in Euro for Spain in 2020\n",
    "total_value_euro_spain_2020 = spain_2020_df['VALUE_IN_EUROS'].sum()\n",
    "\n",
    "# Display the total Value in Euro for Spain in 2020\n",
    "print(f\"Total Value in Euro for Spain in 2020: {total_value_euro_spain_2020}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9fb6ead7-7c69-4d16-8440-e4d6cfb48a85",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
